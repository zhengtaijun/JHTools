# åŸå§‹å¯¼å…¥ä¿æŒä¸å˜
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from rapidfuzz import process, fuzz
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from io import BytesIO
import requests
from PIL import Image
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import json
import re
from functools import lru_cache
from rapidfuzz import process, fuzz



# -------- Robust Excel/HTML/CSV loader --------
# -------- Robust Excel/HTML/CSV loader with auto-convert-to-xlsx --------

def _ensure_xlrd_ok():
    try:
        import xlrd
        parts = tuple(int(p) for p in xlrd.__version__.split(".")[:3])
        if parts < (2, 0, 1):
            raise RuntimeError(f"xlrd {xlrd.__version__} too old; please upgrade to xlrd>=2.0.1")
    except ImportError:
        raise RuntimeError("xlrd not installed; please `pip install xlrd>=2.0.1`")

def _to_xlsx_bytes(df: pd.DataFrame) -> BytesIO:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False)
    bio.seek(0)
    return bio

def read_excel_any(file_obj, return_converted_bytes: bool = False, **kwargs):
    name = (getattr(file_obj, "name", "") or "").lower()

    raw = file_obj.read() if hasattr(file_obj, "read") else file_obj
    if not isinstance(raw, (bytes, bytearray)):
        try:
            file_obj.seek(0)
            raw = file_obj.read()
        except Exception:
            df = pd.read_excel(file_obj, **kwargs)
            return (df, None) if return_converted_bytes else df

    data = bytes(raw)
    head = data[:64]
    sniff = data[:2048].lower()  # å¤š sniff ä¸€ç‚¹ï¼Œä¾¿äºå‘ç° <table> åœ¨å‰

    def as_bio():
        return BytesIO(data)

    # ---------- 1) HTML ä¼ªè£…çš„ Excelï¼š<html / <!doctype / <table éƒ½ç®— ----------
    if (sniff.lstrip().startswith(b"<html")
        or sniff.lstrip().startswith(b"<!doctype html")
        or b"<table" in sniff):          # å…³é”®ï¼šè¡¥ä¸Šè¿™æ¡
        # ç”¨ read_html è§£æï¼Œä¸æŠŠç¬¬ä¸€è¡Œå½“è¡¨å¤´
        tables = pd.read_html(as_bio(), header=None)
        if not tables:
            raise RuntimeError("HTML æ–‡ä»¶ä¸­æœªå‘ç°å¯è§£æçš„è¡¨æ ¼ã€‚è¯·å¯¼å‡ºä¸ºçœŸæ­£çš„ Excelã€‚")
        df = tables[0]

        # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«ä½ çš„æ ‡å‡†å­—æ®µï¼ŒæŠŠç¬¬ä¸€è¡Œæä¸ºåˆ—å
        expected_cols = {
            "datecreated","ordernumber","orderstatus","product_description","size",
            "colour","customername","phone","mobile","deliverymode",
            "publiccomments","qtyrequired","sourcefrom"
        }
        first_row = [str(x).strip() for x in df.iloc[0].tolist()]
        if any(x.lower() in expected_cols for x in first_row):
            df.columns = df.iloc[0]
            df = df.drop(df.index[0]).reset_index(drop=True)

        # ç»Ÿä¸€æˆå­—ç¬¦ä¸²ï¼ˆå’Œ dtype=str æ•ˆæœä¸€è‡´ï¼‰
        df = df.applymap(lambda x: "" if pd.isna(x) else str(x))

        conv = _to_xlsx_bytes(df) if return_converted_bytes else None
        return (df, conv) if return_converted_bytes else df

    # ---------- 2) çœŸ .xlsxï¼ˆZIP å¤´ï¼‰ ----------
    if head.startswith(b"PK\x03\x04"):
        try:
            df = pd.read_excel(as_bio(), engine="openpyxl", **kwargs)
        except Exception:
            df = pd.read_excel(as_bio(), **kwargs)
        return (df, None) if return_converted_bytes else df

    # ---------- 3) çœŸ .xlsï¼ˆOLE2 å¤´ï¼‰æˆ–æ‰©å±•å .xls ----------
    if head.startswith(b"\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1") or name.endswith(".xls"):
        _ensure_xlrd_ok()
        df = pd.read_excel(as_bio(), engine="xlrd", **kwargs)
        return (df, None) if return_converted_bytes else df

    # ---------- 4) CSV/TSV è¯¯æ‰©å±• ----------
    text_sample = data[:4096].decode("utf-8", errors="ignore")
    if ("\t" in text_sample or "," in text_sample) and ("\n" in text_sample or "\r" in text_sample):
        sep = "\t" if text_sample.count("\t") >= text_sample.count(",") else ","
        df = pd.read_csv(BytesIO(data), sep=sep)
        df = df.applymap(lambda x: "" if pd.isna(x) else str(x))
        conv = _to_xlsx_bytes(df) if return_converted_bytes else None
        return (df, conv) if return_converted_bytes else df

    # ---------- 5) å…œåº• ----------
    df = pd.read_excel(as_bio(), **kwargs)
    return (df, None) if return_converted_bytes else df


# ========== GLOBAL CONFIG ==========
favicon = Image.open("favicon.png")
st.set_page_config(
    page_title="JHCH Tools Suite | Andy Wang",
    layout="centered",
    page_icon=favicon
)
st.title("ğŸ› ï¸ Jory Henley CHC â€“ Internal Tools Suite")
st.caption("Â© 2025 â€¢ App author: **Andy Wang**")

# ========== SIDEBAR NAVIGATION ==========
tool = st.sidebar.radio(
    "ğŸ§° Select a tool:",
    ["TRF Volume Calculator", "Order Merge Tool", "Order Merge Tool V2", "Profit Calculator", "List Split", "Image Table Extractor", "Google Sheet Query"],
    index=0
)

# ========== TOOL 1: TRF Volume Calculator ==========
if tool == "TRF Volume Calculator":
    st.subheader("ğŸ“¦ TRF Volume Calculator")
    st.markdown("ğŸ“º [Instructional video](https://youtu.be/S10a3kPEXZg)")

    PRODUCT_INFO_URL = (
        "https://raw.githubusercontent.com/zhengtaijun/JHCH_TRF-Volume/main/product_info.xlsx"
    )

    # ===================== ç»Ÿä¸€æ ‡å‡†åŒ–ä¸åˆ«åå½’ä¸€ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================

    _WS_RE = re.compile(r"\s+")
    _PUNCT_RE = re.compile(r"[^a-z0-9]+")

    ALIASES = {
        "drawer": ["drawers", "drw", "drws"],
        "tallboy": ["tall boy", "tall-boy"],
        "queen": ["qn", "qs", "queen-size", "queen size"],
        "king": ["kg", "ks", "king-size", "king size"],
    }

    def _apply_aliases(tokens):
        out = []
        for t in tokens:
            replaced = False
            for canon, variants in ALIASES.items():
                if t == canon or t in variants:
                    out.append(canon)
                    replaced = True
                    break
            if not replaced:
                out.append(t)
        return out

    def normalize(s: str) -> str:
        s = s.strip().lower()
        s = _PUNCT_RE.sub(" ", s)
        s = _WS_RE.sub(" ", s)
        tokens = s.split()
        tokens = _apply_aliases(tokens)
        return " ".join(tokens)

    def fingerprint(s: str) -> str:
        toks = normalize(s).split()
        return " ".join(sorted(set(toks)))

    @st.cache_data
    def load_product_info_and_build_index():
        resp = requests.get(PRODUCT_INFO_URL)
        resp.raise_for_status()
        df = read_excel_any(BytesIO(resp.content))

        with st.expander("âœ… Product-info file loaded. Click to view columns", expanded=False):
            st.write(df.columns.tolist())

        if {"Product Name", "CBM"} - set(df.columns):
            raise ValueError("`Product Name` and `CBM` columns are required.")

        names = df["Product Name"].fillna("").astype(str).tolist()
        cbms = pd.to_numeric(df["CBM"], errors="coerce").fillna(0).tolist()

        product_dict_raw = dict(zip(names, cbms))

        norm_index = {}
        fp_index = {}
        names_norm_list = []

        for n, c in zip(names, cbms):
            n_norm = normalize(n)
            n_fp = " ".join(sorted(set(n_norm.split())))
            norm_index[n_norm] = c
            fp_index[n_fp] = c
            names_norm_list.append(n_norm)

        return {
            "df": df,
            "product_dict_raw": product_dict_raw,
            "norm_index": norm_index,
            "fp_index": fp_index,
            "names_norm_list": names_norm_list,
            "names_all": names,
            "cbms_all": cbms,
        }

    idx = load_product_info_and_build_index()

    @lru_cache(maxsize=4096)
    def match_product(name: str):
        if not name:
            return None

        raw = idx["product_dict_raw"].get(name)
        if raw is not None:
            return raw

        n_norm = normalize(name)
        got = idx["norm_index"].get(n_norm)
        if got is not None:
            return got

        n_fp = " ".join(sorted(set(n_norm.split())))
        got = idx["fp_index"].get(n_fp)
        if got is not None:
            return got

        tokens = n_norm.split()
        prefix = " ".join(tokens[:3]) if len(tokens) >= 3 else " ".join(tokens)
        if prefix:
            m_prefix = process.extractOne(
                prefix,
                [" ".join(t.split()[:3]) for t in idx["names_norm_list"]],
                scorer=fuzz.token_set_ratio,
                score_cutoff=90,
            )
            if m_prefix:
                _, _, matched_idx = m_prefix
                return idx["cbms_all"][matched_idx]

        m1 = process.extractOne(
            n_norm, idx["names_norm_list"], scorer=fuzz.token_set_ratio, score_cutoff=88
        )
        if m1:
            _, _, matched_idx = m1
            return idx["cbms_all"][matched_idx]

        m2 = process.extractOne(
            n_norm, idx["names_norm_list"], scorer=fuzz.partial_ratio, score_cutoff=85
        )
        if m2:
            _, _, matched_idx = m2
            return idx["cbms_all"][matched_idx]

        return None

    # ===================== ä¸Šä¼ æ–‡ä»¶ + è‡ªåŠ¨è¯†åˆ«åˆ— =====================
    warehouse_file = st.file_uploader("Upload warehouse export (Excel)", type=["xlsx", "xls"])

    df_preview = None
    uploaded_bytes = None
    n_cols = 0

    if warehouse_file:
        try:
            uploaded_bytes = warehouse_file.getvalue()
            df_preview = read_excel_any(BytesIO(uploaded_bytes))
            n_cols = df_preview.shape[1]

            with st.expander("ğŸ“‹ Preview uploaded file & columns", expanded=False):
                st.write(df_preview.head())
                st.write("Columns:", list(df_preview.columns))
        except Exception as e:
            st.error(f"âŒ Failed to read uploaded file: {e}")

    # ---- ç”¨æˆ·å¯ç¼–è¾‘çš„è¡¨å¤´å…³é”®å­—ï¼ˆé»˜è®¤ç”¨ä½ çš„æ ‡é¢˜ï¼‰ ----
    st.markdown("### ğŸ”  Column headers (auto-detect, editable)")
    header_prod = st.text_input("Header for **Product Name**", value="Short Description")
    header_order = st.text_input("Header for **Order Number** (Invoices)", value="Invoices")
    header_qty = st.text_input("Header for **Quantity**", value="Order Qty")
    header_po = st.text_input("Header for **PO Number**", value="PO No")

    # ---- æ ¹æ®è¡¨å¤´å°è¯•è‡ªåŠ¨æ£€æµ‹åˆ—å·ï¼Œç”¨äºç»™ number_input é¢„å¡«å€¼ ----
    def _auto_detect_col(df, header_name, fallback=1):
        if df is None or not header_name:
            return fallback
        cols_lower = [str(c).strip().lower() for c in df.columns]
        target = header_name.strip().lower()
        # å…ˆå®Œå…¨åŒ¹é…
        for i, c in enumerate(cols_lower):
            if c == target:
                return i + 1  # 1-based
        # å†åšåŒ…å«åŒ¹é…
        for i, c in enumerate(cols_lower):
            if target in c:
                return i + 1
        return fallback

    if n_cols == 0:
        max_cols = 50
    else:
        max_cols = n_cols

    auto_prod = _auto_detect_col(df_preview, header_prod, fallback=3)
    auto_order = _auto_detect_col(df_preview, header_order, fallback=7)
    auto_qty = _auto_detect_col(df_preview, header_qty, fallback=8)
    auto_po = _auto_detect_col(df_preview, header_po, fallback=1)

    # ---- æ‰‹åŠ¨åˆ—å·ï¼ˆ1-basedï¼‰ï¼Œé»˜è®¤æ˜¾ç¤ºè‡ªåŠ¨åŒ¹é…åˆ°çš„å€¼ï¼Œä»å¯ä¿®æ”¹ ----
    # ---- æ‰‹åŠ¨åˆ—å·ï¼ˆ1-basedï¼‰ï¼Œé»˜è®¤æ˜¾ç¤ºè‡ªåŠ¨åŒ¹é…åˆ°çš„å€¼ï¼Œä»å¯ä¿®æ”¹ ----
    st.markdown("### #ï¸âƒ£ Column numbers (1-based, optional override)")

    # å¼€å…³ï¼šæ˜¯å¦ç”¨æ‰‹åŠ¨åˆ—å·å¼ºè¡Œ override
    use_manual_cols = st.checkbox(
        "Use manual column numbers to override header detection",
        value=False,
        help="é»˜è®¤å…³é—­ï¼šä»…ä½¿ç”¨ä¸Šé¢çš„è¡¨å¤´æ ‡é¢˜æ¥è¯†åˆ«åˆ—ã€‚å‹¾é€‰åï¼šå¼ºåˆ¶ä½¿ç”¨ä¸‹é¢çš„åˆ—å·ã€‚"
    )

    col_prod = st.number_input(
        "Column # of **Product Name**",
        min_value=1,
        max_value=max_cols,
        value=min(max(auto_prod, 1), max_cols),
    )
    col_order = st.number_input(
        "Column # of **Order Number (Invoices)**",
        min_value=1,
        max_value=max_cols,
        value=min(max(auto_order, 1), max_cols),
    )
    col_qty = st.number_input(
        "Column # of **Quantity**",
        min_value=1,
        max_value=max_cols,
        value=min(max(auto_qty, 1), max_cols),
    )
    col_po = st.number_input(
        "Column # of **PO Number**",
        min_value=1,
        max_value=max_cols,
        value=min(max(auto_po, 1), max_cols),
    )

    # ---- è®¡ç®—æ—¶å®é™…å†³å®šä½¿ç”¨å“ªä¸€åˆ— ----
    def resolve_col_index(df, header_name, manual_1based, auto_1based, use_manual: bool):
        """
        è¿”å› 0-based åˆ—ç´¢å¼•ï¼š

        - å½“ use_manual=True æ—¶ï¼š
            ç›´æ¥ä½¿ç”¨ manual_1based - 1ï¼ˆå¼ºåˆ¶ overrideï¼‰
        - å½“ use_manual=False æ—¶ï¼š
            åªç”¨ header_name åœ¨è¡¨å¤´ä¸­æŸ¥æ‰¾ï¼ˆå…ˆå…¨ç­‰ååŒ…å«ï¼‰ï¼Œ
            å¦‚æœå®Œå…¨æ‰¾ä¸åˆ°ï¼Œåˆ™é€€å›è‡ªåŠ¨ä¾¦æµ‹çš„ auto_1based - 1
        """
        # æ‰‹åŠ¨ override æ¨¡å¼
        if use_manual and manual_1based is not None:
            return int(manual_1based) - 1

        # æ ‡é¢˜åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        if df is not None and header_name:
            cols_lower = [str(c).strip().lower() for c in df.columns]
            target = header_name.strip().lower()
            # å®Œå…¨åŒ¹é…
            for i, c in enumerate(cols_lower):
                if c == target:
                    return i
            # åŒ…å«åŒ¹é…
            for i, c in enumerate(cols_lower):
                if target and target in c:
                    return i

        # æ ‡é¢˜å®Œå…¨æ‰¾ä¸åˆ°ï¼Œé€€å›è‡ªåŠ¨è¯†åˆ«å‡ºæ¥çš„åˆ—å·
        if auto_1based is not None:
            return int(auto_1based) - 1

        raise ValueError(f"Cannot resolve column for header '{header_name}'")


    # ===================== ä½“ç§¯è®¡ç®—æµç¨‹ï¼ˆå¸¦ PO + Invoices + ç²¾ç®€åˆ—ï¼‰ =====================
    # ===================== ä½“ç§¯è®¡ç®—æµç¨‹ï¼ˆå¸¦ PO + Short Description + ç²¾ç®€åˆ—ï¼‰ =====================
    def process_volume_file(file_bytes, prod_idx, qty_idx, inv_idx, po_idx):
        dfw = read_excel_any(BytesIO(file_bytes))

        # å®‰å…¨ä¿æŠ¤ï¼šç´¢å¼•æº¢å‡ºå°±æŠ¥é”™
        ncols = dfw.shape[1]
        for idx_check, label in [
            (prod_idx, "Product Name"),
            (qty_idx, "Quantity"),
            (inv_idx, "Invoices"),
            (po_idx, "PO No"),
        ]:
            if idx_check < 0 or idx_check >= ncols:
                raise ValueError(f"Column index for {label} out of range.")

        # åŸºç¡€åˆ—
        product_series = dfw.iloc[:, prod_idx].fillna("").astype(str)
        product_names = product_series.tolist()
        quantities = pd.to_numeric(dfw.iloc[:, qty_idx], errors="coerce").fillna(0)

        inv_series = dfw.iloc[:, inv_idx] if inv_idx is not None else pd.Series([""] * len(dfw))
        po_series = dfw.iloc[:, po_idx] if po_idx is not None else pd.Series([""] * len(dfw))

        # åˆå¹¶ PO No ä¸ Invoices åˆ°ä¸€ä¸ªå•å…ƒæ ¼ï¼ˆPO åœ¨å‰ï¼Œç”¨é€—å·éš”å¼€ï¼Œå¹¶åœ¨æ•°å­—å‰åŠ  "PO"ï¼‰
        merged_ref = []
        for po, inv in zip(po_series, inv_series):
            # --- å¤„ç† PO ä¸ºçº¯æ–‡æœ¬ï¼Œå»æ‰ .0ã€å°æ•°ã€ç©ºæ ¼ç­‰ ---
            po_s = "" if pd.isna(po) else str(po).strip()

            if po_s:
                # å¦‚æœæ˜¯çº¯æ•°å­—æˆ–æ•°å­—.0ï¼Œå˜æˆçº¯æ•´æ•°å­—ç¬¦ä¸²
                if re.fullmatch(r"\d+(\.0+)?", po_s):
                    po_s = po_s.split(".", 1)[0]

                po_s = po_s.strip()

                # ç»Ÿä¸€åŠ ä¸Šâ€œPOâ€å‰ç¼€ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                if not po_s.upper().startswith("PO"):
                    po_s = f"PO{po_s}"

            inv_s = "" if pd.isna(inv) else str(inv).strip()

            if po_s and inv_s:
                merged_ref.append(f"{po_s}, {inv_s}")
            elif po_s:
                merged_ref.append(po_s)
            else:
                merged_ref.append(inv_s)


        # ---- å¹¶è¡Œåšä½“ç§¯åŒ¹é… ----
        total = len(product_names)
        volumes = [None] * total

        def worker(start: int, end: int):
            out = []
            for i in range(start, end):
                nm = product_names[i].strip()
                vol = match_product(nm) if nm else None
                out.append(vol)
            return out

        with ThreadPoolExecutor(max_workers=4) as pool:
            chunk = max(total // 4, 1)
            futures = [
                pool.submit(worker, i * chunk, (i + 1) * chunk if i < 3 else total)
                for i in range(4)
            ]
            pos = 0
            for f in futures:
                batch = f.result()
                volumes[pos : pos + len(batch)] = batch
                pos += len(batch)

        # åªä¿ç•™ä¸‰åˆ— + Volume & Total Volume
        df_res = pd.DataFrame(
            {
                "PO/Invoice": merged_ref,                     # åˆå¹¶ PO No + Invoices
                "Short Description": product_series,          # äº§å“ååˆ—
                "Order Qty": quantities,                      # æ•°é‡
            }
        )

        df_res["Volume"] = pd.to_numeric(pd.Series(volumes), errors="coerce").fillna(0)
        df_res["Total Volume"] = df_res["Volume"] * df_res["Order Qty"]

        # æœ€åä¸€è¡Œæ±‡æ€» Total Volume
        summary = pd.DataFrame(
            {
                "PO/Invoice": [""],
                "Short Description": [""],
                "Order Qty": [""],
                "Volume": [""],
                "Total Volume": [df_res["Total Volume"].sum()],
            }
        )

        df_final = pd.concat([df_res, summary], ignore_index=True)
        return df_final


    # ===================== è§¦å‘è®¡ç®—ä¸ä¸‹è½½ =====================
    if warehouse_file and uploaded_bytes and st.button("Calculate volume"):
        if df_preview is None:
            st.error("âŒ File not loaded correctly, please re-upload.")
        else:
            with st.spinner("Processingâ€¦"):
                try:
                    prod_idx = resolve_col_index(df_preview, header_prod, col_prod, auto_prod, use_manual_cols)
                    qty_idx  = resolve_col_index(df_preview, header_qty,  col_qty,  auto_qty,  use_manual_cols)
                    inv_idx  = resolve_col_index(df_preview, header_order, col_order, auto_order, use_manual_cols)
                    po_idx   = resolve_col_index(df_preview, header_po,   col_po,   auto_po,   use_manual_cols)


                    result_df = process_volume_file(
                        uploaded_bytes, prod_idx, qty_idx, inv_idx, po_idx
                    )

                    buffer = BytesIO()
                    with pd.ExcelWriter(buffer, engine="xlsxwriter") as writer:
                        result_df.to_excel(writer, index=False)
                    buffer.seek(0)

                    st.success("âœ… Volume calculation complete.")
                    st.dataframe(result_df.head(50), use_container_width=True)

                    st.download_button(
                        "ğŸ“¥ Download Excel",
                        buffer,
                        file_name="TRF_Volume_Result.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
                except Exception as e:
                    st.error(f"âŒ Error: {e}")

    pass

# ========== TOOL 2: Order Merge Tool ==========
# ========== TOOL 2: Order Freight Compare & Volume (New) ==========
elif tool == "Order Merge Tool":
    st.subheader("ğŸšš Advanced Freight Compare + Volume")
    st.markdown("""
    æœ¬å·¥å…·å¯¹æ¯” **ä»“åº“å‘è´§è¡¨ï¼ˆAï¼‰** ä¸ **è‡ªåˆ¶è®¢å•è¡¨ï¼ˆBï¼‰**ï¼š

    - è‡ªåŠ¨è¯†åˆ«ï¼šA è¡¨å« `First Receipt Date` åˆ—ï¼›B è¡¨ä¸å«  
    - A è¡¨éœ€è¦åˆ—ï¼š`PO No`, `Short Description`, `Order Qty`  
    - B è¡¨éœ€è¦åˆ—ï¼š`Product_Description`, `SourceFrom`, `qtyRequired`, `OrderNumber`  
    - å¯¹æ¯”ç»“æœåˆ†å››ç§æƒ…å†µå¹¶åˆå¹¶ä¸ºä¸€ä¸ªè¡¨ï¼š  
      1ï¸âƒ£ PO + äº§å“ + æ•°é‡å®Œå…¨åŒ¹é…ï¼ˆä»“åº“ & æˆ‘æ–¹ä¸€è‡´ï¼‰  
      2ï¸âƒ£ åªæœ‰ A æœ‰ï¼ˆä»“åº“å¤šåšäº† / æˆ‘æ–¹æ¼å•ï¼‰  
      3ï¸âƒ£ åªæœ‰ B æœ‰ï¼ˆæˆ‘æ–¹ä¸‹å•äº† / ä»“åº“æ¼åšï¼‰  
      4ï¸âƒ£ åŒæ–¹éƒ½æ²¡æœ‰ POï¼ˆåº—å†…åº“å­˜ / å±•å“ï¼Œæ— éœ€ä»“åº“å‘è´§ï¼‰  
    - ä½¿ç”¨äº§å“åç§°åŒ¹é… `product_info.xlsx` ä¸­ CBMï¼Œè®¡ç®—ä½“ç§¯ä¸æ€»å’Œ
    """)

    # ---------- å…±ç”¨ï¼šproduct_info ä½“ç§¯å­—å…¸ & æ¨¡ç³ŠåŒ¹é… ----------
    PRODUCT_INFO_URL = (
        "https://raw.githubusercontent.com/zhengtaijun/JHCH_TRF-Volume/main/product_info.xlsx"
    )

    _WS_RE = re.compile(r"\s+")
    _PUNCT_RE = re.compile(r"[^a-z0-9]+")

    ALIASES = {
        "drawer": ["drawers", "drw", "drws"],
        "tallboy": ["tall boy", "tall-boy"],
        "queen": ["qn", "qs", "queen-size", "queen size"],
        "king": ["kg", "ks", "king-size", "king size"],
    }

    def _apply_aliases(tokens):
        out = []
        for t in tokens:
            replaced = False
            for canon, variants in ALIASES.items():
                if t == canon or t in variants:
                    out.append(canon)
                    replaced = True
                    break
            if not replaced:
                out.append(t)
        return out

    def normalize_name(s: str) -> str:
        s = (str(s) if s is not None else "").strip().lower()
        s = _PUNCT_RE.sub(" ", s)
        s = _WS_RE.sub(" ", s)
        tokens = s.split()
        tokens = _apply_aliases(tokens)
        return " ".join(tokens)

    @st.cache_data
    def load_product_info_index():
        resp = requests.get(PRODUCT_INFO_URL)
        resp.raise_for_status()
        df = read_excel_any(BytesIO(resp.content))

        if {"Product Name", "CBM"} - set(df.columns):
            raise ValueError("`Product Name` å’Œ `CBM` åˆ—åœ¨ product_info.xlsx ä¸­æ˜¯å¿…é¡»çš„ã€‚")

        names = df["Product Name"].fillna("").astype(str).tolist()
        cbms = pd.to_numeric(df["CBM"], errors="coerce").fillna(0).tolist()

        product_dict_raw = dict(zip(names, cbms))

        norm_index = {}
        fp_index = {}
        names_norm_list = []

        for n, c in zip(names, cbms):
            n_norm = normalize_name(n)
            n_fp = " ".join(sorted(set(n_norm.split())))
            norm_index[n_norm] = c
            fp_index[n_fp] = c
            names_norm_list.append(n_norm)

        return {
            "df": df,
            "product_dict_raw": product_dict_raw,
            "norm_index": norm_index,
            "fp_index": fp_index,
            "names_norm_list": names_norm_list,
            "names_all": names,
            "cbms_all": cbms,
        }

    idx_vol = load_product_info_index()

    @lru_cache(maxsize=4096)
    def match_product_cmb(name: str):
        """æ ¹æ®äº§å“ååŒ¹é… CBMï¼ˆå¤åˆ¶è‡ª TRF Volume é€»è¾‘ï¼‰"""
        if not name:
            return None

        # 0. åŸæ–‡ç²¾ç¡®
        raw = idx_vol["product_dict_raw"].get(name)
        if raw is not None:
            return raw

        n_norm = normalize_name(name)

        # 1. è§„èŒƒåŒ–ç²¾ç¡®
        got = idx_vol["norm_index"].get(n_norm)
        if got is not None:
            return got

        # 2. æŒ‡çº¹ç²¾ç¡®
        n_fp = " ".join(sorted(set(n_norm.split())))
        got = idx_vol["fp_index"].get(n_fp)
        if got is not None:
            return got

        # 3a. å‰ç¼€æ¨¡ç³Š
        tokens = n_norm.split()
        prefix = " ".join(tokens[:3]) if len(tokens) >= 3 else " ".join(tokens)
        if prefix:
            m_prefix = process.extractOne(
                prefix,
                [" ".join(t.split()[:3]) for t in idx_vol["names_norm_list"]],
                scorer=fuzz.token_set_ratio,
                score_cutoff=90,
            )
            if m_prefix:
                _, _, matched_idx = m_prefix
                return idx_vol["cbms_all"][matched_idx]

        # 3b. å…¨åæ¨¡ç³Š
        m1 = process.extractOne(
            n_norm, idx_vol["names_norm_list"], scorer=fuzz.token_set_ratio, score_cutoff=88
        )
        if m1:
            _, _, matched_idx = m1
            return idx_vol["cbms_all"][matched_idx]

        # 3c. partial å…œåº•
        m2 = process.extractOne(
            n_norm, idx_vol["names_norm_list"], scorer=fuzz.partial_ratio, score_cutoff=85
        )
        if m2:
            _, _, matched_idx = m2
            return idx_vol["cbms_all"][matched_idx]

        return None

    # ---------- å·¥å…·å‡½æ•°ï¼šåˆ—åè‡ªåŠ¨åŒ¹é… ----------
    def find_col(df: pd.DataFrame, targets, required=True):
        """
        åœ¨ df.columns ä¸­æŸ¥æ‰¾åˆ—åï¼Œtargets å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼›
        å…ˆå…¨ç­‰ï¼Œå†åŒ…å«ï¼›æ‰¾ä¸åˆ°ä¸” required=True åˆ™æŠ¥é”™ã€‚
        è¿”å› 0-based indexã€‚
        """
        if isinstance(targets, str):
            targets = [targets]
        cols_lower = [str(c).strip().lower() for c in df.columns]
        for t in targets:
            t_low = t.lower()
            # å®Œå…¨åŒ¹é…
            for i, c in enumerate(cols_lower):
                if c == t_low:
                    return i
        for t in targets:
            t_low = t.lower()
            # åŒ…å«åŒ¹é…
            for i, c in enumerate(cols_lower):
                if t_low in c:
                    return i
        if required:
            raise ValueError(f"æ‰¾ä¸åˆ°åˆ—ï¼š{targets}")
        return None

    # ---------- å·¥å…·å‡½æ•°ï¼šè§„èŒƒ PO ç¼–å·ä¸ºæ–‡æœ¬ "POxxxx" ----------
    RE_FLOAT_INT = re.compile(r"^\s*(\d+)(?:\.0+)?\s*$")
    RE_HASH_PO = re.compile(r"#\s*(\d+)")
    RE_NEED_ORDER = re.compile(r"(on[- ]order|pending)", re.IGNORECASE)

    def normalize_po(value):
        """æŠŠå„ç§æ ¼å¼ï¼ˆ1234 / 1234.0 / 'PO1234'ï¼‰ç»Ÿä¸€ä¸º 'PO1234' æ–‡æœ¬ï¼›ç©ºè¿”å› ''ã€‚"""
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return ""
        s = str(value).strip()
        if not s:
            return ""
        # çº¯æ•°å­—æˆ–ç±»ä¼¼ 1234.0
        m = RE_FLOAT_INT.match(s)
        if m:
            num = m.group(1)
            return f"PO{num}"
        # å·²æœ‰ PO å‰ç¼€
        s_u = s.upper()
        if s_u.startswith("PO"):
            # å»æ‰å¯èƒ½çš„ PO00123.0 è¿™ç±»
            tail = s_u[2:].strip()
            m2 = RE_FLOAT_INT.match(tail)
            if m2:
                return f"PO{m2.group(1)}"
            return s_u
        # å…¶ä»–æƒ…å†µå½“æ™®é€šæ–‡æœ¬ï¼ŒåŠ  PO
        return "PO" + s

    # ---------- ä¸Šä¼ ä¸¤ä¸ªæ–‡ä»¶ ----------
    fileA = st.file_uploader("ğŸ“„ Upload **Warehouse file A** (with 'First Receipt Date')", type=["xlsx", "xls"], key="freight_A")
    fileB = st.file_uploader("ğŸ“„ Upload **Internal order file B**", type=["xlsx", "xls"], key="freight_B")

    if fileA and fileB and st.button("ğŸ” Compare & Calculate Volume"):
        try:
            # è¯»å–
            dfA = read_excel_any(fileA)
            dfB = read_excel_any(fileB)

            colsA = [str(c) for c in dfA.columns]
            colsB = [str(c) for c in dfB.columns]

            has_first_A = any("first receipt date" in str(c).lower() for c in colsA)
            has_first_B = any("first receipt date" in str(c).lower() for c in colsB)

            # è‡ªåŠ¨çº æ­£ A/B è§’è‰²ï¼šè°å« First Receipt Date è°å°±æ˜¯ A
            if has_first_B and not has_first_A:
                dfA, dfB = dfB, dfA
                colsA, colsB = colsB, colsA
                st.info("â„¹ï¸ æ£€æµ‹åˆ°ç¬¬äºŒä¸ªæ–‡ä»¶æ‰åŒ…å« `First Receipt Date`ï¼Œå·²è‡ªåŠ¨å°†å…¶è§†ä¸ºè¡¨æ ¼ Aï¼ˆä»“åº“è¡¨ï¼‰ã€‚")
            elif not has_first_A:
                st.warning("âš ï¸ æœªåœ¨ä»»ä¸€æ–‡ä»¶ä¸­å‘ç° `First Receipt Date` åˆ—ï¼Œè¯·ç¡®è®¤æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æ­£ç¡®ã€‚")

            # ---- æ‰¾ A è¡¨åˆ—ï¼šPO No / Short Description / Order Qty ----
            idxA_po = find_col(dfA, ["PO No", "PONo", "PO_Number"])
            idxA_desc = find_col(dfA, ["Short Description", "Short_Description", "Description"])
            idxA_qty = find_col(dfA, ["Order Qty", "OrderQty", "Qty"])

            # ---- æ‰¾ B è¡¨åˆ—ï¼šProduct_Description / SourceFrom / qtyRequired / OrderNumber ----
            idxB_desc = find_col(dfB, ["Product_Description", "Product Description", "Product"])
            idxB_source = find_col(dfB, ["SourceFrom", "Source From"])
            idxB_qty = find_col(dfB, ["qtyRequired", "Qty Required", "Order Qty", "OrderQty"])
            idxB_order = find_col(dfB, ["OrderNumber", "Order Number", "OrderNo"])

            # é¢„è§ˆ
            with st.expander("ğŸ‘€ Preview A (warehouse)", expanded=False):
                st.write(dfA.head())
            with st.expander("ğŸ‘€ Preview B (internal)", expanded=False):
                st.write(dfB.head())

            # ---------- é¢„å¤„ç† A è¡¨ ----------
            rowsA = []
            for i, r in dfA.iterrows():
                po_raw = r.iloc[idxA_po]
                po_norm = normalize_po(po_raw)
                has_po = bool(po_norm)

                desc = str(r.iloc[idxA_desc]) if not pd.isna(r.iloc[idxA_desc]) else ""
                desc_norm = normalize_name(desc)

                qty_raw = r.iloc[idxA_qty]
                try:
                    qty = int(float(qty_raw)) if str(qty_raw).strip() != "" else 0
                except Exception:
                    qty = 0

                rowsA.append(
                    dict(
                        idx=i,
                        po=po_norm,
                        has_po=has_po,
                        desc=desc,
                        desc_norm=desc_norm,
                        qty=qty,
                    )
                )

            # ---------- é¢„å¤„ç† B è¡¨ ----------
            rowsB = []
            for i, r in dfB.iterrows():
                src = "" if pd.isna(r.iloc[idxB_source]) else str(r.iloc[idxB_source])
                src_low = src.lower()

                ## åˆ¤æ–­æ˜¯å¦éœ€è¦è®¢è´§ï¼šSourceFrom ä¸­åŒ…å« On-Order æˆ– Pending
                need_order = bool(RE_NEED_ORDER.search(src_low))
                m_po = RE_HASH_PO.search(src)
                po_norm = ""
                has_po = False
                if need_order and m_po:
                    po_norm = normalize_po(m_po.group(1))
                    has_po = bool(po_norm)

                desc = "" if pd.isna(r.iloc[idxB_desc]) else str(r.iloc[idxB_desc])
                desc_norm = normalize_name(desc)

                qty_raw = r.iloc[idxB_qty]
                try:
                    qty = int(float(qty_raw)) if str(qty_raw).strip() != "" else 0
                except Exception:
                    qty = 0

                order_no = "" if pd.isna(r.iloc[idxB_order]) else str(r.iloc[idxB_order]).strip()

                rowsB.append(
                    dict(
                        idx=i,
                        po=po_norm,
                        has_po=has_po,
                        desc=desc,
                        desc_norm=desc_norm,
                        qty=qty,
                        order_no=order_no,
                    )
                )

            # ---------- æŒ‰ (PO, desc_norm, qty) å»ºç«‹ key ----------
            # ---------- å…ˆæŒ‰ (PO, desc_norm) èšåˆï¼Œæ–¹ä¾¿å¯¹æ¯”æ•°é‡ ----------
            # Aï¼šåªçœ‹æœ‰ PO çš„è¡Œ
            A_group = {}
            for ra in rowsA:
                if not ra["has_po"]:
                    continue
                key = (ra["po"], ra["desc_norm"])
                if key not in A_group:
                    A_group[key] = {
                        "po": ra["po"],
                        "desc": ra["desc"],
                        "qty": 0,
                    }
                A_group[key]["qty"] += ra["qty"]

            # Bï¼šåªçœ‹æœ‰ PO çš„è¡Œï¼ŒåŒæ—¶èšåˆ OrderNumberï¼ˆå¤šä¸ªå°±åˆå¹¶ï¼‰
            B_group = {}
            for rb in rowsB:
                if not rb["has_po"]:
                    continue
                key = (rb["po"], rb["desc_norm"])
                if key not in B_group:
                    B_group[key] = {
                        "po": rb["po"],
                        "desc": rb["desc"],
                        "qty": 0,
                        "orders": [],
                    }
                B_group[key]["qty"] += rb["qty"]
                if rb["order_no"]:
                    B_group[key]["orders"].append(rb["order_no"])

            # ---------- Part 1 & Part 2ï¼šäº¤é›†é‡Œçš„ â€œå®Œå…¨åŒ¹é…â€ å’Œ â€œæ•°é‡ä¸ä¸€è‡´â€ ----------
            part1 = []  # å®Œå…¨åŒ¹é…
            part2 = []  # æ•°é‡ä¸ä¸€è‡´ï¼ˆPO + äº§å“ä¸€è‡´ï¼Œqty ä¸åŒï¼‰

            common_keys = set(A_group.keys()) & set(B_group.keys())
            for key in common_keys:
                ga = A_group[key]
                gb = B_group[key]
                qty_a = ga["qty"]
                qty_b = gb["qty"]

                # åˆå¹¶ PO + OrderNumber
                po_cell = ga["po"]
                if gb["orders"]:
                    # å¤šä¸ªè®¢å•å·ç”¨é€—å·æ‹¼åœ¨åé¢
                    po_cell = po_cell + ", " + ", ".join(gb["orders"])

                product = gb["desc"] or ga["desc"]  # ä¼˜å…ˆç”¨ B çš„æè¿°

                if qty_a == qty_b:
                    part1.append(
                        dict(
                            Category="1. Match (A & B)",
                            PO_Order=po_cell,
                            Product=product,
                            Qty=qty_a,
                        )
                    )
                else:
                    # æ•°é‡ä¸ä¸€è‡´ï¼šä»ç„¶ä»¥ A çš„æ•°é‡ä½œä¸ºä½“ç§¯è®¡ç®—çš„åŸºå‡†
                    part2.append(
                        dict(
                            Category="2. Qty mismatch (PO & product same)",
                            PO_Order=po_cell,
                            Product=product,
                            Qty=qty_a,
                            # å¦‚æœåé¢ä½ æƒ³çœ‹ B çš„æ•°é‡ï¼Œä¹Ÿå¯ä»¥é¢å¤–åŠ ä¸€åˆ— Qty_B
                            # Qty_B=qty_b,
                        )
                    )

            # ---------- Part 3ï¼šåªåœ¨ A æœ‰ï¼ˆä»“åº“æœ‰ï¼Œæˆ‘ä»¬æ²¡ä¸‹å• / ä¸‹å°‘äº†ï¼‰ ----------
            part3 = []
            onlyA_keys = set(A_group.keys()) - set(B_group.keys())
            for key in onlyA_keys:
                ga = A_group[key]
                part3.append(
                    dict(
                        Category="3. Only in A (warehouse extra)",
                        PO_Order=ga["po"],
                        Product=ga["desc"],
                        Qty=ga["qty"],
                    )
                )

            # ---------- Part 4ï¼šåªåœ¨ B æœ‰ POï¼ˆæˆ‘ä»¬ä¸‹å•ï¼Œä»“åº“æ²¡åšï¼‰ ----------
            part4 = []
            onlyB_keys = set(B_group.keys()) - set(A_group.keys())
            for key in onlyB_keys:
                gb = B_group[key]
                po_cell = gb["po"]
                if gb["orders"]:
                    po_cell = po_cell + ", " + ", ".join(gb["orders"])
                part4.append(
                    dict(
                        Category="4. Only in B (our order, warehouse missing)",
                        PO_Order=po_cell,
                        Product=gb["desc"],
                        Qty=gb["qty"],
                    )
                )

            # ---------- Part 5ï¼šåŒæ–¹éƒ½æ²¡ POï¼ˆåº—å†…åº“å­˜ / å±•å“ï¼‰ ----------
            # è¿™é‡Œä»æŒ‰ B è¡¨ä¸­ â€œæ²¡æœ‰ PO çš„è¡Œâ€ è§†ä¸ºåº—å†…åº“å­˜
            part5 = []
            for rb in rowsB:
                if rb["has_po"]:
                    continue
                part5.append(
                    dict(
                        Category="5. No PO (store stock / display)",
                        PO_Order=rb["order_no"],
                        Product=rb["desc"],
                        Qty=rb["qty"],
                    )
                )

            # ---------- åˆå¹¶äº”éƒ¨åˆ†ï¼Œè®¡ç®— Volume ----------
            all_rows = part1 + part2 + part3 + part4 + part5
            if not all_rows:
                st.error("æœªæ‰¾åˆ°ä»»ä½•è®°å½•ï¼Œè¯·æ£€æŸ¥ä¸¤ä¸ªè¡¨æ ¼å†…å®¹æ˜¯å¦æ­£ç¡®ã€‚")
            else:
                df_result = pd.DataFrame(all_rows)

                # ä½“ç§¯åŒ¹é…
                vols = []
                for name in df_result["Product"].tolist():
                    v = match_product_cmb(name or "")
                    vols.append(v if v is not None else 0)
                df_result["Volume"] = pd.to_numeric(pd.Series(vols), errors="coerce").fillna(0)

                df_result["Qty"] = pd.to_numeric(df_result["Qty"], errors="coerce").fillna(0)
                df_result["Total Volume"] = df_result["Volume"] * df_result["Qty"]

                total_volume_sum = df_result["Total Volume"].sum()

                # æœ€åä¸€è¡Œæ±‡æ€»
                summary_row = {
                    "Category": "TOTAL",
                    "PO_Order": "",
                    "Product": "",
                    "Qty": "",
                    "Volume": "",
                    "Total Volume": total_volume_sum,
                }
                df_final = pd.concat([df_result, pd.DataFrame([summary_row])], ignore_index=True)

                st.success(f"âœ… Completed. Total rows: {len(df_result)}, Total Volume: **{total_volume_sum:.3f} mÂ³**")

                # åˆ†éƒ¨åˆ†å±•ç¤º
                st.markdown("### ğŸ“Š Part 1 â€“ å®Œå…¨åŒ¹é…ï¼ˆä»“åº“ & æˆ‘æ–¹ä¸€è‡´ï¼‰")
                st.dataframe(
                    df_result[df_result["Category"] == "1. Match (A & B)"][["PO_Order","Product","Qty","Volume","Total Volume"]],
                    use_container_width=True,
                )

                st.markdown("### ğŸ“Š Part 2 â€“ æ•°é‡ä¸ä¸€è‡´ï¼ˆPO & äº§å“ç›¸åŒï¼‰")
                st.dataframe(
                    df_result[df_result["Category"] == "2. Qty mismatch (PO & product same)"][["PO_Order","Product","Qty","Volume","Total Volume"]],
                    use_container_width=True,
                )

                st.markdown("### ğŸ“Š Part 3 â€“ ä»… A æœ‰ï¼ˆä»“åº“å¤šåš / æˆ‘æ–¹æ¼å•ï¼‰")
                st.dataframe(
                    df_result[df_result["Category"] == "3. Only in A (warehouse extra)"][["PO_Order","Product","Qty","Volume","Total Volume"]],
                    use_container_width=True,
                )

                st.markdown("### ğŸ“Š Part 4 â€“ ä»… B æœ‰ POï¼ˆæˆ‘æ–¹ä¸‹å• / ä»“åº“æ¼åšï¼‰")
                st.dataframe(
                    df_result[df_result["Category"] == "4. Only in B (our order, warehouse missing)"][["PO_Order","Product","Qty","Volume","Total Volume"]],
                    use_container_width=True,
                )

                st.markdown("### ğŸ“Š Part 5 â€“ æ—  POï¼ˆåº—å†…åº“å­˜ / å±•å“ï¼‰")
                st.dataframe(
                    df_result[df_result["Category"] == "5. No PO (store stock / display)"][["PO_Order","Product","Qty","Volume","Total Volume"]],
                    use_container_width=True,
                )


                # å¯¼å‡ºåˆ°å¸¦é¢œè‰²çš„ Excel
                out = BytesIO()
                with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
                    df_final.to_excel(writer, index=False, sheet_name="Compare")
                    workbook = writer.book
                    worksheet = writer.sheets["Compare"]

                    fmt_part1 = workbook.add_format({"bg_color": "#C6EFCE"})  # ç»¿ï¼šå®Œå…¨åŒ¹é…
                    fmt_part2 = workbook.add_format({"bg_color": "#F8CBAD"})  # æ©™ï¼šæ•°é‡ä¸ä¸€è‡´
                    fmt_part3 = workbook.add_format({"bg_color": "#FFEB9C"})  # é»„ï¼šåªåœ¨ A
                    fmt_part4 = workbook.add_format({"bg_color": "#FFC7CE"})  # çº¢ï¼šåªåœ¨ B
                    fmt_part5 = workbook.add_format({"bg_color": "#D9E1F2"})  # è“ï¼šæ—  PO
                    fmt_total = workbook.add_format({"bold": True})

                    cat_col_idx = df_final.columns.get_loc("Category")
                    for row_idx in range(1, len(df_final) + 1):
                        cat = df_final.iloc[row_idx - 1, cat_col_idx]
                        fmt = None
                        if cat.startswith("1. "):
                            fmt = fmt_part1
                        elif cat.startswith("2. Qty"):
                            fmt = fmt_part2
                        elif cat.startswith("3. Only in A"):
                            fmt = fmt_part3
                        elif cat.startswith("4. Only in B"):
                            fmt = fmt_part4
                        elif cat.startswith("5. No PO"):
                            fmt = fmt_part5
                        elif cat == "TOTAL":
                            fmt = fmt_total
                        if fmt:
                            worksheet.set_row(row_idx, None, fmt)


                out.seek(0)
                st.download_button(
                    "ğŸ“¥ Download Excel (with colored sections)",
                    out,
                    file_name="freight_compare_volume.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )

        except Exception as e:
            st.error(f"âŒ Error: {e}")
        pass

# ========== TOOL 3: Order Merge Tool V2 ==========
elif tool == "Order Merge Tool V2":
    st.subheader("ğŸ“‹ Order Merge Tool V2")
    st.markdown("ğŸ“˜ [View User Guide](https://github.com/zhengtaijun/JHTools/blob/main/instruction%20v2.png)")

    st.info(
        "ğŸ“¢ å…¬å‘Šï¼šæœ¬å·¥å…·å°†æ—§è¡¨ï¼ˆæŒ‰äº§å“åˆ†è¡Œï¼‰æ•´ç†ä¸ºæ¯ä¸ª **OrderNumber** åªä¿ç•™ä¸€è¡Œçš„æ–°è¡¨ã€‚\n\n"
        "- äº§å“åè‡ªåŠ¨å»é‡åˆå¹¶ï¼ˆProduct_Description + Size + Colourï¼‰\n"
        "- ç¬¬14åˆ—è¾“å‡º POï¼ˆå½¢å¦‚ PO3513ï¼‰ï¼Œå¿½ç•¥ â€œ2 Availableâ€\n"
        "- ç¬¬15åˆ—è¾“å‡º Itemsï¼ˆå½¢å¦‚ `qty*åˆå¹¶åäº§å“å`ï¼Œå¤šä»¶é€—å·åˆ†éš”ï¼‰\n"
        "- **ç¬¬äºŒåˆ— DateCreated è¾“å‡ºä¸º `yyyy/mm/dd`ï¼ˆæŒ‰æ–œæ ä½ç½®æš´åŠ›é‡æ’ï¼‰**\n"
        "- é¢„è§ˆæ”¯æŒ**ä¸€é”®å¤åˆ¶è¡¨æ ¼ï¼ˆä¸å«è¡¨å¤´ï¼‰**\n"
    )

    import streamlit.components.v1 as components
    BUILD_ID = "OMTV2-2025-10-15-02"

    file = st.file_uploader("Upload the Excel file (old layout)", type=["xlsx","xls"], key="order_merge_v2")

    RE_PO = re.compile(r'(?:PO:|<strong>PO:</strong>)\s*#?\s*(\d+)', re.IGNORECASE)
    RE_WS = re.compile(r'\s+')

    REQUIRED_COLS = [
        "DateCreated","OrderNumber","OrderStatus","Product_Description","Size","Colour",
        "CustomerName","Phone","Mobile","DeliveryMode","PublicComments","qtyRequired","SourceFrom"
    ]

    # ----------------- æš´åŠ›æ—¥æœŸè§£æï¼šåªçœ‹ç¬¬ä¸€ä¸ª a/b/cï¼Œç„¶åé‡æ’ä¸º yyyy/mm/dd -----------------
# 1) æš´åŠ›æ—¥æœŸè§£æï¼ˆä¿æŒä¸å˜ï¼‰
    RE_DMY = re.compile(r'(\d{1,4})\s*/\s*(\d{1,2})\s*/\s*(\d{2,4})')

    def brutal_extract_ymd(value):
        s = str(value).strip()
        m = RE_DMY.search(s)
        if not m:
            return None
        a, b, c = m.groups()   # a=day, b=month, c=yearï¼ˆåŸå§‹æ˜¯ dd/mm/yyyyï¼‰
        day = int(a)
        month = int(b)
        year = int(c) if len(c) == 4 else int("20" + c)
        return (year, month, day)   # è¿”å› (yyyy, mm, dd)

    # 2) åªåœ¨â€œä¸æ˜¯ yyyy/mm/ddâ€æ—¶æ‰é‡æ’
    RE_YMD_FINAL = re.compile(r'^\s*\d{4}/\d{1,2}/\d{1,2}\s*$')

    def brutal_format_ymd(value):
        s = str(value).strip()
        # å·²ç»æ˜¯ yyyy/mm/dd â†’ ç›´æ¥è¿”å›ï¼Œé¿å…äºŒæ¬¡é‡æ’
        if RE_YMD_FINAL.match(s):
            return s
        t = brutal_extract_ymd(s)
        if not t:
            return s if value is not None else ""
        y, m, d = t
        return f"{y}/{m}/{d}"   # ä¸è¡¥é›¶ï¼š2025/10/5

    def brutal_min_date(series):
        tuples = [brutal_extract_ymd(v) for v in series]
        tuples = [t for t in tuples if t is not None]
        if not tuples:
            # æ²¡æŠ“åˆ°ä»»ä½• a/b/cï¼Œå°±å›é€€è¾“å‡ºé¦–ä¸ªéç©ºåŸæ–‡ï¼ˆæˆ–ç©ºï¼‰
            for v in series:
                if str(v).strip():
                    return brutal_format_ymd(v)
            return ""
        y, m, d = min(tuples)           # æŒ‰ (yyyy, mm, dd) å–æœ€æ—©
        return f"{y}/{m}/{d}"


    # ----------------- å…¶å®ƒå·¥å…·å‡½æ•° -----------------
    def clean_str(s):
        if pd.isna(s):
            return ""
        s = str(s)
        s = re.sub(r"<[^>]*>", "", s)            # å» HTML æ ‡ç­¾
        s = RE_WS.sub(" ", s).strip()
        return s

    def contains_ci(hay, needle):
        return bool(needle) and needle.lower() in hay.lower()

    def merge_product_name(prod, size, colour):
        p = clean_str(prod)
        s = clean_str(size)
        c = clean_str(colour)

        parts = [p] if p else []
        if s and not contains_ci(p, s):
            parts.append(s)
        merged = " ".join(parts) if parts else ""
        if c and not contains_ci(merged, c):
            merged = (merged + (" - " if merged else "") + c)
        return merged

    def fmt_qty_name(qty, name):
        if not name:
            return ""
        if pd.isna(qty) or str(qty).strip() == "":
            return name
        try:
            q = float(qty)
            q_int = int(q)
            q_str = str(q_int) if abs(q - q_int) < 1e-9 else str(q)
        except Exception:
            q_str = str(qty).strip()
        return f"{q_str}*{name}"

    def extract_pos(value):
        """æå– POxxxxï¼›å¯¹ '2 Available' ç±»ä¸å¤„ç†ã€‚"""
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return []
        text = str(value)
        if re.search(r"\b\d+\s+Available\b", text, flags=re.IGNORECASE):
            return []
        matches = RE_PO.findall(text)
        return [f"PO{m}" for m in matches]

    def first_nonempty(values):
        for v in values:
            if isinstance(v, str) and v.strip():
                return v.strip()
            if pd.notna(v) and str(v).strip():
                return str(v).strip()
        return ""

    # ----------------- ä¸»æ•´åˆé€»è¾‘ -----------------
    def consolidate(df: pd.DataFrame) -> pd.DataFrame:
        # ç¡®ä¿ç¼ºå¤±åˆ—å­˜åœ¨
        for col in REQUIRED_COLS:
            if col not in df.columns:
                df[col] = pd.NA

        # é¢„å¤„ç†
        df["_MergedName"] = df.apply(
            lambda r: merge_product_name(r["Product_Description"], r["Size"], r["Colour"]), axis=1
        )
        df["_ItemLine"] = [fmt_qty_name(q, n) for q, n in zip(df["qtyRequired"], df["_MergedName"])]
        df["_POs"] = df["SourceFrom"].apply(extract_pos)

        def merge_phones(phone, mobile):
            def normalize_phone(v):
                if pd.isna(v):
                    return ""
                s = str(v).strip()
                if s.endswith(".0"):
                    s = s[:-2]
                return s

            parts = [normalize_phone(phone), normalize_phone(mobile)]
            parts = [p for p in parts if p]
            seen, unique = set(), []
            for x in parts:
                if x not in seen:
                    seen.add(x)
                    unique.append(x)
            return ", ".join(unique)

        df["_Phones"] = [merge_phones(p, m) for p, m in zip(df["Phone"], df["Mobile"])]

        rows = []
        for order, g in df.groupby("OrderNumber", dropna=False):
            g = g.copy()

            # ç¬¬4åˆ—ï¼šHomeDelivery â†’ ä»»ä¸€è¡Œä¸º 'home' åˆ™ç½® 1ï¼Œå¦åˆ™ 'pickup'
            delivery_vals = [str(x).strip().lower() for x in g["DeliveryMode"].tolist()]
            home_flag = 1 if any(x == "home" for x in delivery_vals) else "pickup"

            # ç¬¬12åˆ—ï¼šAwaitingPayment æ ‡è®°
            status_vals = [str(x).strip() for x in g["OrderStatus"].tolist() if str(x).strip()]
            awaiting_flag = 1 if any(x.lower() == "awaiting payment" for x in status_vals) else ""

            # ç¬¬15åˆ—ï¼šItems
            items = [x for x in g["_ItemLine"].tolist() if x]
            items_text = ", ".join(items)

            # ç¬¬14åˆ—ï¼šPOsï¼ˆæ‰å¹³+å»é‡ï¼‰
            po_list, seen_po = [], set()
            for sub in g["_POs"].tolist():
                for x in sub:
                    if x not in seen_po:
                        seen_po.add(x)
                        po_list.append(x)
            po_text = ", ".join(po_list)

            # ç¬¬7åˆ—ï¼šContactPhonesï¼ˆå»é‡ï¼‰
            phone_opts = [x for x in g["_Phones"].tolist() if x]
            seen_ph, phone_unique = set(), []
            for x in phone_opts:
                if x not in seen_ph:
                    seen_ph.add(x)
                    phone_unique.append(x)
            phones_text = ", ".join(phone_unique)

            # ç¬¬13åˆ—ï¼šPublicCommentsï¼ˆå»é‡ï¼‰
            comments_vals = [clean_str(x) for x in g["PublicComments"].tolist() if clean_str(x)]
            seen_c, comments_unique = set(), []
            for x in comments_vals:
                if x not in seen_c:
                    seen_c.add(x)
                    comments_unique.append(x)
            comments_text = " | ".join(comments_unique)

            # ç¬¬2åˆ—ï¼šDateCreatedï¼ˆæŒ‰æ–œæ ä¸‰æ®µæš´åŠ›é‡æ’ï¼Œå¹¶åœ¨ç»„å†…å–æœ€æ—©ä¸€ä¸ªï¼‰â†’ yyyy/mm/dd
            date_value = brutal_min_date(g["DateCreated"])

            # ç¬¬6åˆ—ï¼šCustomerNameï¼ˆé¦–ä¸ªéç©ºï¼‰
            customer = first_nonempty(g["CustomerName"].tolist())

            row = {
                "OrderNumber": order,             # 1
                "DateCreated": date_value,        # 2
                "Col3": "",                       # 3
                "HomeDelivery": home_flag,        # 4
                "Col5": "",                       # 5
                "CustomerName": customer,         # 6
                "ContactPhones": phones_text,     # 7
                "Col8": "", "Col9": "", "Col10": "", "Col11": "",  # 8~11 ç©º
                "AwaitingPayment": awaiting_flag, # 12
                "PublicComments": comments_text,  # 13
                "POs": po_text,                   # 14
                "Items": items_text,              # 15
            }
            rows.append(row)

        out = pd.DataFrame(rows)

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
        for h in [
            "OrderNumber","DateCreated","Col3","HomeDelivery","Col5","CustomerName","ContactPhones",
            "Col8","Col9","Col10","Col11","AwaitingPayment","PublicComments","POs","Items"
        ]:
            if h not in out.columns:
                out[h] = ""

        # ä¿é™©ï¼šå†ç»Ÿä¸€ç”¨æš´åŠ›æ ¼å¼å™¨
        out["DateCreated"] = out["DateCreated"].astype(str).apply(brutal_format_ymd)

        # åˆ—ä½é‡æ’
        out = out[
            ["OrderNumber","DateCreated","Col3","HomeDelivery","Col5",
             "CustomerName","ContactPhones","Col8","Col9","Col10","Col11",
             "AwaitingPayment","PublicComments","POs","Items"]
        ]
        return out

    def validate_columns(df: pd.DataFrame):
        return [c for c in REQUIRED_COLS if c not in df.columns]

    if file:
        try:
            # è¯»å–ï¼šè‡ªåŠ¨å…¼å®¹ .xlsx / .xls / HTMLä¼ªExcel / è¯¯æ‰©å±•CSV/TSV
            raw_df, converted = read_excel_any(file, return_converted_bytes=True)

            # è‹¥è‡ªåŠ¨å‘ç”Ÿäº†æ ¼å¼è½¬æ¢ï¼Œç»™å‡ºæç¤ºä¸ä¸‹è½½æŒ‰é’®
            if converted:
                st.info("ğŸ” æ£€æµ‹åˆ° HTML/CSV ä¼ªè£…çš„ Excelï¼Œå·²è‡ªåŠ¨è½¬æ¢ä¸ºçœŸå® .xlsxã€‚")
                st.download_button(
                    "ğŸ“¥ ä¸‹è½½è‡ªåŠ¨è½¬æ¢çš„ .xlsx",
                    converted,
                    file_name="converted.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

            # åˆ—æ ¡éªŒ
            missing = validate_columns(raw_df)
            if missing:
                st.error("âŒ ç¼ºå°‘ä»¥ä¸‹å¿…è¦åˆ—ï¼Œè¯·åœ¨åŸè¡¨ä¸­è¡¥é½åå†ä¸Šä¼ ï¼š\n\n- " + "\n- ".join(missing))
            else:
                with st.spinner("Processingâ€¦"):
                    merged = consolidate(raw_df)

                # é¢„è§ˆï¼ˆæ˜¾ç¤ºå‰ 50 è¡Œï¼‰
                st.success(f"âœ… å¤„ç†å®Œæˆï¼Œå…± {len(merged)} æ¡è®¢å•ï¼ˆæ¯ä¸ª OrderNumber ä¸€è¡Œï¼‰ã€‚")
                st.dataframe(merged.head(50), use_container_width=True)

                # â€”â€” ä¸€é”®å¤åˆ¶ï¼ˆä¸å«è¡¨å¤´ï¼Œå¤åˆ¶æ•´è¡¨ï¼‰ â€”â€”
                tsv_no_header = merged.to_csv(sep="\t", header=False, index=False)
                components.html(f"""
                    <textarea id="mergedTSV" style="position:absolute;left:-10000px;top:-10000px">{tsv_no_header}</textarea>
                    <button onclick="(function(){{
                        var t=document.getElementById('mergedTSV');
                        t.select();
                        document.execCommand('copy');
                        alert('âœ… å·²å¤åˆ¶å…¨è¡¨ï¼ˆä¸å«è¡¨å¤´ï¼‰ï¼Œå¯ç›´æ¥ç²˜è´´åˆ° Excel/Google Sheets');
                    }})()" style="margin:8px 0; padding:.5em 1em; border-radius:8px; border:1px solid #ccc;">
                        ğŸ“‹ Copy table (no headers)
                    </button>
                """, height=40)

                # ä¸‹è½½ç»“æœï¼ˆExcelï¼‰
                out_io = BytesIO()
                with pd.ExcelWriter(out_io, engine="xlsxwriter") as writer:
                    merged.to_excel(writer, index=False, sheet_name="Consolidated")
                out_io.seek(0)

                st.download_button(
                    "ğŸ“¥ Download Merged Excel",
                    data=out_io,
                    file_name="order_merge_v2.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        except RuntimeError as e:
            st.error(f"âŒ {e}")
        except Exception as e:
            st.error(f"âŒ Error: {e}")
    pass




# ========== TOOL 3: Profit Calculator ==========
elif tool == "Profit Calculator":
    st.subheader("ğŸ’° Profit Calculator")
    st.caption("All data is calculated locally Â· Multi-product supported Â· Updated by Andy Wang")

    # äº§å“æ•°é‡
    num_products = st.number_input("Number of products", min_value=1, max_value=20, value=1)
    cols = st.columns(num_products)

    # æ¯ä¸ªäº§å“è¾“å…¥ä¸å«ç¨æˆæœ¬ & æœåŠ¡è´¹ç™¾åˆ†æ¯”
    base_costs = []
    service_rates = []
    for i in range(num_products):
        st.markdown(f"**Product {i+1}**")
        col1, col2 = st.columns([2, 1])
        with col1:
            cost = st.number_input(
                f"Base cost (excl. GST) â€“ Product {i+1}",
                min_value=0.0,
                step=0.01,
                format="%.2f",
                value=None,
                placeholder="E.g. 289.75",
                key=f"base_cost_{i}"
            ) or 0.0
        with col2:
            rate = st.radio(
                f"Service Fee â€“ P{i+1}",
                ["15%", "5%"],
                horizontal=True,
                key=f"rate_{i}"
            )
            rate_val = 0.15 if rate == "15%" else 0.05

        base_costs.append(cost)
        service_rates.append(rate_val)

    # è®¡ç®—æ€»æˆæœ¬ï¼ˆå«æœåŠ¡è´¹å’ŒGSTï¼‰
    total_base_cost = sum(base_costs)
    service_fees = [c * r for c, r in zip(base_costs, service_rates)]
    cost_with_service = [c + s for c, s in zip(base_costs, service_fees)]
    total_cost_excl_gst = sum(cost_with_service)
    total_cost_incl_gst = total_cost_excl_gst * 1.15

    # è¿è´¹ä½“ç§¯è¾“å…¥
    total_volume = st.number_input("Total volume (mÂ³)", min_value=0.0, step=0.0001, format="%.3f", placeholder="E.g. 0.75") or 0.0
    shipping_unit_price = st.number_input("Shipping unit price (NZD/mÂ³, excl. GST)", min_value=0.0, step=0.01, format="%.2f", value=150.0)
    shipping_cost_excl_gst = total_volume * shipping_unit_price
    shipping_cost_incl_gst = shipping_cost_excl_gst * 1.15

    # å”®ä»·è¾“å…¥ï¼ˆå«ç¨ï¼‰
    sale_price = st.number_input("Input sale price (GST included, NZD)", min_value=0.0, step=0.01, format="%.2f", value=None, placeholder="E.g. 1200") or 0.0

    # å…¶ä»–æˆæœ¬
    rent = sale_price * 0.10

    # æ±‡æ€»
    total_expense = total_cost_incl_gst + shipping_cost_incl_gst + rent
    profit_with_gst = sale_price - total_expense
    profit_no_gst = profit_with_gst / 1.15 if profit_with_gst else 0.0

    def pct(n):
        return f"{(n/(sale_price or 1)*100):.2f}%" if sale_price else "-"

    # è¾“å‡ºè¡¨æ ¼
    result_rows = [
        ["Base Cost (Sum)", total_base_cost, pct(total_base_cost)],
        ["Service Fees", sum(service_fees), pct(sum(service_fees))],
        ["Product Total Cost (incl. GST)", total_cost_incl_gst, pct(total_cost_incl_gst)],
        ["Shipping (incl. GST)", shipping_cost_incl_gst, pct(shipping_cost_incl_gst)],
        ["Rent (10%)", rent, pct(rent)],
        ["Total Cost", total_expense, pct(total_expense)],
        ["Profit (incl. GST)", profit_with_gst, pct(profit_with_gst)],
        ["Profit (excl. GST)", profit_no_gst, ""]
    ]
    df_res = pd.DataFrame(result_rows, columns=["Item", "Amount (NZD)", "Ratio to Sale Price"])
    df_res["Amount (NZD)"] = df_res["Amount (NZD)"].map(lambda x: f"{x:.2f}")
    st.table(df_res)

    # æ˜ç»†è¯´æ˜
    with st.expander("Calculation details"):
        st.markdown(f"""
- **Base Cost Total** = {total_base_cost:.2f} NZD
- **Service Fee Total** = {sum(service_fees):.2f} NZD
- **Cost w/ Service Fee (excl. GST)** = {total_cost_excl_gst:.2f} NZD
- **Cost w/ GST** = {total_cost_incl_gst:.2f} NZD
- **Shipping (excl. GST)** = {shipping_cost_excl_gst:.2f} NZD
- **Shipping (incl. GST)** = {shipping_cost_incl_gst:.2f} NZD
- **Rent** = {rent:.2f} NZD
- **Total Expense** = {total_expense:.2f} NZD
- **Profit (incl. GST)** = {profit_with_gst:.2f} NZD
- **Profit (excl. GST)** = {profit_no_gst:.2f} NZD
        """)

    # é¥¼å›¾
    if sale_price > 0:
        fig, ax = plt.subplots()
        ax.pie(
            [total_cost_incl_gst, shipping_cost_incl_gst, rent, max(profit_with_gst, 0)],
            labels=["Product Cost", "Shipping", "Rent", "Profit"],
            autopct="%1.1f%%",
            startangle=90
        )
        ax.axis("equal")
        st.pyplot(fig)

    # å¯¼å‡º Excel
    if st.button("Export results to Excel"):
        out = BytesIO()
        df_res.to_excel(out, index=False)
        out.seek(0)
        st.download_button(
            "ğŸ“¥ Download Excel",
            out,
            file_name="profit_result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    pass

# ========== TOOL 4: List Split ==========
elif tool == "List Split":
    st.subheader("ğŸ“„ List Split")
    st.markdown("Paste copied table data with order number and products. Format: `2*Chair,1*Table`")

    pasted_text = st.text_area("Paste your copied data below (from Excel):")

    if st.button("ğŸ” Analyze pasted content") and pasted_text:
        try:
            from io import StringIO
            # è¯»å–ç²˜è´´çš„åˆ¶è¡¨ç¬¦åˆ†éš”æ–‡æœ¬ï¼›å…¨éƒ¨æŒ‰å­—ç¬¦ä¸²å¤„ç†ï¼Œé¿å…æ•°å€¼è¢«è½¬æˆ 1.0 ä¹‹ç±»
            df_input = pd.read_csv(StringIO(pasted_text), sep="\t", header=None, dtype=str)

            st.write("âœ… Preview of parsed input:")
            st.dataframe(df_input, use_container_width=True)

            # ç»Ÿä¸€æ¸…æ´—ï¼šæŠŠ None/NaN/'nan' ç­‰è½¬ä¸ºç©ºä¸²ï¼Œä¸”å»é¦–å°¾ç©ºæ ¼
            def _fmt_cell(v):
                if v is None:
                    return ""
                s = str(v).strip()
                return "" if s.lower() in ("nan", "none") else s

            records = []

            for _, row in df_input.iterrows():
                # è®¢å•å·ï¼šç¬¬ä¸€åˆ—ï¼ˆè‹¥åªæœ‰1åˆ—ä¹Ÿèƒ½å–åˆ°ï¼‰
                order_id = _fmt_cell(row.iloc[0]) if len(row) >= 1 else ""

                # ä¾›åº”å•†è®¢è´§å·ï¼šå€’æ•°ç¬¬äºŒåˆ—ï¼ˆéœ€è¦è‡³å°‘2åˆ—æ‰æœ‰ï¼‰
                supplier_code = _fmt_cell(row.iloc[-2]) if len(row) >= 2 else ""

                # åˆå¹¶æˆ â€œä¾›åº”å•†è®¢è´§å·//è®¢å•å·â€ï¼Œè‹¥ä¾›åº”å•†è®¢è´§å·ç¼ºå¤±åˆ™ä»…ç”¨è®¢å•å·
                combined_order_ref = f"{supplier_code}//{order_id}" if supplier_code else order_id

                # äº§å“æ¸…å•ï¼šæœ€åä¸€åˆ—ï¼Œå½¢å¦‚ "2*Chair,1*Table"
                product_str = _fmt_cell(row.iloc[-1]) if len(row) >= 1 else ""
                items = [item.strip() for item in product_str.split(',') if '*' in item]

                for item in items:
                    try:
                        qty_str, name = item.split('*', 1)
                        qty_str = _fmt_cell(qty_str)
                        name = _fmt_cell(name)
                        if not name:
                            continue
                        # å…¼å®¹ "2" / "2.0" / " 2 " ç­‰
                        qty = int(float(qty_str)) if qty_str else 0
                        records.append({
                            'order': combined_order_ref,  # â† åˆå¹¶åçš„ â€œä¾›åº”å•†è®¢è´§å·//è®¢å•å·â€
                            'name': name,
                            'qty': qty
                        })
                    except Exception:
                        st.warning(f"âš ï¸ Skipped malformed item: {item}")

            if records:
                # å›ºå®šåˆ—é¡ºåº
                df_result = pd.DataFrame(records)[['order', 'name', 'qty']]

                st.info("ğŸ§© å·²å°†å€’æ•°ç¬¬äºŒåˆ—è¯†åˆ«ä¸ºã€ä¾›åº”å•†è®¢è´§å·ã€ï¼Œå¹¶ä¸ç¬¬ä¸€åˆ—ã€è®¢å•å·ã€åˆå¹¶ä¸ºï¼š**ä¾›åº”å•†è®¢è´§å·//è®¢å•å·**")
                st.success("âœ… Processing completed.")
                st.dataframe(df_result, use_container_width=True)

                to_download = BytesIO()
                df_result.to_excel(to_download, index=False)
                to_download.seek(0)

                st.download_button("ğŸ“¥ Download Excel", to_download, file_name="parsed_list.xlsx")
            else:
                st.error("No valid records found. Please check your input.")
        except Exception as e:
            st.error(f"âŒ Error processing input: {e}")
            pass

# ========== TOOL 5: Image Table Extractor ==========
elif tool == "Image Table Extractor":
    st.subheader("ğŸ–¼ï¸ Excel Screenshot to Table")
    st.markdown("Paste (Ctrl+V) or drag a screenshot of an Excel table. Supported formats: JPG, PNG")

    from PIL import Image
    import pytesseract
    import re
    import base64
    import json
    import streamlit.components.v1 as components

    uploaded_image = st.file_uploader("Upload Screenshot", type=["jpg", "jpeg", "png"])

    pasted_image_bytes = st.session_state.get("pasted_image", None)

    # Paste listener
    pasted_image_holder = st.empty()
    components.html('''
        <script>
        document.addEventListener('paste', async function (event) {
            const items = (event.clipboardData || window.clipboardData).items;
            for (const item of items) {
                if (item.type.indexOf('image') === 0) {
                    const file = item.getAsFile();
                    const reader = new FileReader();
                    reader.onload = function(event) {
                        const base64Image = event.target.result;
                        const pyMsg = {type: "paste_image", data: base64Image};
                        window.parent.postMessage(pyMsg, "*");
                    };
                    reader.readAsDataURL(file);
                }
            }
        });
        </script>
    ''', height=0)

    # Paste upload handler
    pasted_json = st.query_params.get("pasted_image")
    if pasted_json:
        try:
            imgdata = base64.b64decode(pasted_json[0].split(",")[-1])
            st.session_state.pasted_image = imgdata
            st.query_params.clear()  # Clear param after use
        except:
            st.warning("Failed to decode pasted image.")

    image = None
    if uploaded_image:
        image = Image.open(uploaded_image)
        st.image(image, caption="Uploaded image", use_column_width=True)
    elif pasted_image_bytes:
        image = Image.open(BytesIO(pasted_image_bytes))
        st.image(image, caption="Pasted image", use_column_width=True)

    if image:
        with st.spinner("Running OCR... please wait..."):
            raw_text = pytesseract.image_to_string(image)
            lines = raw_text.strip().split("\n")
            rows = [re.split(r'\t+|\s{2,}', line.strip()) for line in lines if line.strip()]
            max_len = max((len(row) for row in rows), default=0)
            rows = [row + [''] * (max_len - len(row)) for row in rows]
            df = pd.DataFrame(rows)

        st.success("âœ… OCR complete. Here's the extracted table:")
        st.dataframe(df)

        # Download Excel
        out = BytesIO()
        df.to_excel(out, index=False, header=False)
        out.seek(0)
        st.download_button(
            "ğŸ“¥ Download as Excel",
            out,
            file_name="extracted_table.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # Copy to clipboard button (tab-separated)
        def df_to_tsv(df):
            return '\n'.join(['\t'.join(map(str, row)) for row in df.values.tolist()])

        tsv_string = df_to_tsv(df)

        components.html(f'''
            <textarea id="tsv" style="position:absolute;left:-1000px">{tsv_string}</textarea>
            <button onclick="copyTSV()">ğŸ“‹ Copy Table (for Excel/Sheets)</button>
            <script>
            function copyTSV() {{
                const t = document.getElementById("tsv");
                t.select();
                document.execCommand("copy");
                alert("âœ… Table copied to clipboard. You can now paste into Excel or Google Sheets.");
            }}
            </script>
        ''', height=50)
    else:
        st.info("Please upload or paste a screenshot of a table to begin.")
        pass
# ========== TOOL 6: Order Check ==========
elif tool == "Google Sheet Query":
    st.subheader("ğŸ” Google Sheet æŸ¥è¯¢å·¥å…·")
    st.markdown("ä½¿ç”¨ Google Sheet ä½œä¸ºæ•°æ®åº“ï¼Œå›ºå®šæå–ç¬¬ 1ã€2ã€4ã€6ã€7ã€13ã€15 åˆ—")

    SHEET_ID = "17twAYxaakAIbDhQvFR6FdgUVFHxBJlL5w8rrC7gpCu8"
    SHEET_NAME = "Sheet1"

    @st.cache_data
    def load_sheet_data():
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds_dict = json.loads(st.secrets["GOOGLE_CREDENTIALS"])
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        client = gspread.authorize(creds)
        sheet = client.open_by_key(SHEET_ID).worksheet(SHEET_NAME)

        all_data = sheet.get_all_values()
        if not all_data:
            return pd.DataFrame()

        # å›ºå®šé€‰æ‹©ç¬¬1, 2, 4, 6, 7, 13, 15åˆ— (æ³¨æ„ Pythonç´¢å¼•ä»0å¼€å§‹)
        col_indices = [0, 1, 3, 5, 6, 12, 14]  # A, B, D, F, G, M, O
        headers = all_data[0]
        rows = all_data[1:]

        # å¤„ç†ï¼šåªä¿ç•™æŒ‡å®šåˆ—
        selected_headers = [headers[i] if i < len(headers) else f"Col{i+1}" for i in col_indices]
        selected_rows = [[row[i] if i < len(row) else "" for i in col_indices] for row in rows]

        df = pd.DataFrame(selected_rows, columns=selected_headers)
        return df

    try:
        df = load_sheet_data()
        if df.empty:
            st.warning("âš ï¸ è¡¨æ ¼ä¸ºç©ºæˆ–æ•°æ®åŠ è½½å¤±è´¥ã€‚")
        else:
            st.success("âœ… è¡¨æ ¼åŠ è½½æˆåŠŸï¼")
            
            with st.expander("ğŸ“‹ æ˜¾ç¤ºå…¨éƒ¨æ•°æ®ï¼ˆå¯é€‰ï¼‰", expanded=False):
                st.dataframe(df, use_container_width=True)

            query = st.text_input("ğŸ” è¾“å…¥å…³é”®è¯ï¼ˆæ¨¡ç³ŠåŒ¹é…æ‰€æœ‰åˆ—ï¼‰:")

            if query:
                filtered = df[df.apply(lambda row: row.astype(str).str.contains(query, case=False).any(), axis=1)]
                st.markdown(f"ğŸ” **å…±æ‰¾åˆ° {len(filtered)} æ¡åŒ¹é…ç»“æœï¼š**")
                st.dataframe(filtered, use_container_width=True)
            else:
                st.info("è¯·è¾“å…¥å…³é”®è¯å¼€å§‹æŸ¥è¯¢ã€‚")

    except Exception as e:
        st.error(f"âŒ åŠ è½½ Google Sheet å¤±è´¥ï¼š{e}")
